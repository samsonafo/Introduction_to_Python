{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot calculate **the accuracy for a regression model.**\n",
    "\n",
    "The skill or performance of a regression model must be reported as an error in those predictions.\n",
    "\n",
    "This makes sense if you think about it. If you are predicting a numeric value like a height or a dollar amount, you don’t want to know if the model predicted the value exactly (this might be intractably difficult in practice); instead, we want to know how close the predictions were to the expected values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Mean Absolute Error(MAE)**\n",
    "\n",
    "MAE calculates the **absolute** difference between actual and predicted values.\n",
    "\n",
    "To better understand, let’s take an example you have input data and output data and use Linear Regression, which draws a best-fit line.\n",
    "\n",
    "Now you have to find the MAE of your model which is basically a mistake made by the model known as an error. Now find the difference between the actual value and predicted value that is an absolute error but we have to find the mean absolute of the complete dataset.\n",
    "\n",
    "so, sum all the errors and divide them by a total number of observations And this is MAE. And we aim to get a minimum MAE because this is a loss.\n",
    "\n",
    "\n",
    "![Mean Absolute Error](./img/mae.png)\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"MAE\",mean_absolute_error(y_test,y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Mean Squared Error(MSE)**\n",
    "\n",
    "MSE is a most used and very simple metric with a little bit of change in mean absolute error. Mean squared error states that finding the squared difference between actual and predicted value.\n",
    "\n",
    "So, above we are finding the absolute difference and here we are finding the squared difference.\n",
    "\n",
    "What actually the MSE represents? It represents the squared distance between actual and predicted values. we perform squared to avoid the cancellation of negative terms and it is the benefit of MSE.\n",
    "\n",
    "![Mean Squared Error](./img/mse.png)\n",
    "\n",
    "```\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"MSE\",mean_squared_error(y_test,y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) **Root Mean Squared Error(RMSE)**\n",
    "\n",
    "As RMSE is clear by the name itself, that it is a simple square root of mean squared error.\n",
    "\n",
    "![Root Mean Squared Error](./img/rmse.png)\n",
    "\n",
    "To implement RMSE, we use the NumPy square root function:\n",
    "\n",
    "```\n",
    "\"RMSE\" = np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) **R Squared (R2)**\n",
    "\n",
    "R2 score is a metric that tells the performance of your model, not the loss in an absolute sense that how many wells did your model perform.\n",
    "\n",
    "In contrast, MAE and MSE depend on the context as we have seen whereas the R2 score is independent of context.\n",
    "\n",
    "So, with help of R squared we have a baseline model to compare a model which none of the other metrics provides. The same we have in classification problems which we call a threshold which is fixed at 0.5. So basically R2 squared calculates how must regression line is better than a mean line.\n",
    "\n",
    "Hence, R2 squared is also known as Coefficient of Determination or sometimes also known as Goodness of fit.\n",
    "\n",
    "![R Squared](./img/r2.png)\n",
    "\n",
    "Now, how will you interpret the R2 score? suppose If the R2 score is zero then the above regression line by mean line is equal means 1 so 1-1 is zero. So, in this case, both lines are overlapping means model performance is worst, It is not capable to take advantage of the output column.\n",
    "\n",
    "Now the second case is when the R2 score is 1, it means when the division term is zero and it will happen when the regression line does not make any mistake, it is perfect. In the real world, it is not possible.\n",
    "\n",
    "So we can conclude that as our regression line moves towards perfection, R2 score move towards one. And the model performance improves.\n",
    "\n",
    "The normal case is when the R2 score is between zero and one like 0.8 which means your model is capable to explain 80 per cent of the variance of data.\n",
    "\n",
    "```\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "print(r2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) **Adjusted R Squared**\n",
    "\n",
    "The disadvantage of the R2 score is while adding new features in data the R2 score starts increasing or remains constant but it never decreases because It assumes that while adding more data variance of data increases.\n",
    "\n",
    "But the problem is when we add an irrelevant feature in the dataset then at that time R2 sometimes starts increasing which is incorrect.\n",
    "\n",
    "Hence, To control this situation Adjusted R Squared came into existence.\n",
    "\n",
    "![Adjusted R Squared](./img/adjusted-r-squared.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as K increases by adding some features so the denominator will decrease, n-1 will remain constant. \n",
    "\n",
    "R2 score will remain constant or will increase slightly so the complete answer will increase and when we subtract this from one then the resultant score will decrease. so this is the case when we add an irrelevant feature in the dataset.\n",
    "\n",
    "And if we add a relevant feature then the R2 score will increase and 1-R2 will decrease heavily and the denominator will also decrease so the complete term decreases, and on subtracting from one the score increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There's also the RMSPE -- Root Mean Square Percentage Error.**\n",
    "**MAPE -- Mean Absolute Percentage Error**\n",
    "\n",
    "\n",
    "Source: Analytics Vidhya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Support Vector Machine” (SVM) is a supervised machine learning algorithm that can be used for both classification or regression challenges. \n",
    "\n",
    "However,  it is mostly used in classification problems. \n",
    "\n",
    "In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is a number of features you have) with the value of each feature being the value of a particular coordinate. \n",
    "\n",
    "Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\n",
    "\n",
    "![Support Vector Machines](./img/SVM_1.png)\n",
    "\n",
    "**Note: The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N — the number of features) that distinctly classifies the data points.**\n",
    "\n",
    "Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes. \n",
    "\n",
    "Also, the dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. It becomes difficult to imagine when the number of features exceeds 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does SVM Work?\n",
    "\n",
    "**Identify the right hyper-plane (Scenario-1):** Here, we have three hyper-planes (A, B, and C). Now, identify the right hyper-plane to classify stars and circles.\n",
    "\n",
    "![Scenario 1](./img/SVM_1_1.png)\n",
    "\n",
    "You need to remember a thumb rule to identify the right hyper-plane: “Select the hyper-plane which segregates the two classes better”. In this scenario, hyper-plane “B” has excellently performed this job.\n",
    "\n",
    "**Identify the right hyper-plane (Scenario-2):** Here, we have three hyper-planes (A, B, and C) and all are segregating the classes well. Now, How can we identify the right hyper-plane?\n",
    "\n",
    "![Scenario 2](./img/SVM_3.png)\n",
    "\n",
    "Above, you can see that the margin for hyper-plane C is high as compared to both A and B. \n",
    "\n",
    "Hence, we name the right hyper-plane as C. Another lightning reason for selecting the hyper-plane with higher margin is robustness. If we select a hyper-plane having low margin then there is high chance of miss-classification.\n",
    "\n",
    "**Identify the right hyper-plane (Scenario-3):** Hint: Use the rules as discussed in previous section to identify the right hyper-plane\n",
    "\n",
    "![Scenario 3](./img/SVM_5.png)\n",
    "\n",
    "Some of you may have selected the hyper-plane B as it has higher margin compared to A. But, here is the catch, SVM selects the hyper-plane which classifies the classes accurately prior to maximizing margin. Here, hyper-plane B has a classification error and A has classified all correctly. Therefore, the right hyper-plane is A.\n",
    "\n",
    "**Can we classify two classes (Scenario-4)?:** Below, I am unable to segregate the two classes using a straight line, as one of the stars lies in the territory of other(circle) class as an outlier. \n",
    "\n",
    "![Scenario 4](./img/SVM_61.png)\n",
    "\n",
    "As I have already mentioned, one star at other end is like an outlier for star class. The SVM algorithm has a feature to ignore outliers and find the hyper-plane that has the maximum margin. Hence, we can say, SVM classification is robust to outliers.\n",
    "\n",
    "![Scenario 4](./img/SVM_71.png)\n",
    "\n",
    "**Find the hyper-plane to segregate to classes (Scenario-5):** In the scenario below, we can’t have linear hyper-plane between the two classes, so how does SVM classify these two classes? Till now, we have only looked at the linear hyper-plane.\n",
    "\n",
    "![Scenario 5](./img/SVM_8.png)\n",
    "\n",
    "SVM can solve this problem. Easily! It solves this problem by introducing additional feature. Here, we will add a new feature z=x^2+y^2. Now, let’s plot the data points on axis x and z:\n",
    "\n",
    "![Scenario 6](./img/SVM_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros and Cons associated with SVM\n",
    "\n",
    "Pros:\n",
    "\n",
    "- It works really well with a clear margin of separation\n",
    "- It is effective in high dimensional spaces.\n",
    "- It is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "- It uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "\n",
    "Cons:\n",
    "\n",
    "- It doesn’t perform well when we have large data set because the required training time is higher\n",
    "- It also doesn’t perform very well, when the data set has more noise i.e. target classes are overlapping\n",
    "- SVM doesn’t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation. - It is included in the related SVC method of Python scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing SVM's in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#classification\n",
    "\n",
    "#import the needed libraries\n",
    "from sklearn.svm import SVC  #SVM-classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets        ## imports datasets from scikit-learn - the boston dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "##Load and return the iris dataset\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "#split dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "clf = SVC(kernel='linear')  #instantiate the Support Vector Classifier\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1404341259560358\n"
     ]
    }
   ],
   "source": [
    "#Regression\n",
    "\n",
    "from sklearn.svm import SVR    #SVM Regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn import datasets        ## imports datasets from scikit-learn - the boston dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "##Load and return the boston house-prices dataset (regression).\n",
    "X, y = datasets.load_boston(return_X_y=True)\n",
    "\n",
    "#split dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "clr = SVR(kernel='linear')    #instantiate the Support Vector Regressor\n",
    "clr.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred = clr.predict(X_test)\n",
    "print(mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.435908618391455\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5986037082794649\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.167713306373673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print(mean_absolute_percentage_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Model Completion and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we compare the Logistic Regression to the Support Vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise:\n",
    "\n",
    "- Load the promotions dataset. \n",
    "- clean the data!\n",
    "- Split the data on train,test split on 80-20 ratio.\n",
    "- set the recurrence-events column as the dependent column.\n",
    "- Build a Logistic Regression model and Support Vector Machine Classifier to predict the dependent column - 'promoted or not!\n",
    "- calculate and compare the F1 score of your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
